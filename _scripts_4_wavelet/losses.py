# E:\LD-CT SR\_scripts_4_wavelet\losses.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import pywt
import numpy as np

class WaveletLoss(nn.Module):
    """
    ë…¼ë¬¸ ê¸°ë°˜ Wavelet Loss with Soft Thresholding
    Reference: "ë³µë¶€ CT ì˜ìƒì˜ í™”ì§ˆ ê°œì„  ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬" (2023)
    
    í•µì‹¬ ê°œì„ :
    1. Soft Thresholding ì ìš© (ë…¼ë¬¸ì˜ SoT ë°©ì‹)
    2. Multi-level decomposition (3-levelë¡œ ê°•í™”)
    3. Adaptive threshold per level
    4. Blurring ë°©ì§€ ê°•í™”
    """
    def __init__(self, wavelet='haar', threshold=50, levels=3, normalize_threshold=True):
        super().__init__()
        self.wavelet = wavelet
        self.threshold = threshold  # ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ìµœì ê°’: 50
        self.levels = levels  # 3-levelë¡œ ë” ì„¸ë°€í•˜ê²Œ
        self.normalize_threshold = normalize_threshold
        
        print(f"ğŸ“Š WaveletLoss Initialized:")
        print(f"   Wavelet: {wavelet}")
        print(f"   Threshold: {threshold}")
        print(f"   Levels: {levels}")
        print(f"   â†’ Blurring ë°©ì§€ ê°•í™” ëª¨ë“œ")
    
    def soft_threshold(self, coeffs, threshold):
        """
        Soft Thresholding (SoT) - ë…¼ë¬¸ì˜ ìˆ˜ì‹ (2)
        
        w'(i) = 0           if |w(i)| < T
              = w(i) - T   if w(i) â‰¥ T  
              = w(i) + T   if w(i) â‰¤ -T
        
        íš¨ê³¼: ì„ê³„ê°’ë³´ë‹¤ ì‘ì€ ê³„ìˆ˜(ë…¸ì´ì¦ˆ)ëŠ” 0ìœ¼ë¡œ, í° ê³„ìˆ˜(ì‹ í˜¸)ëŠ” ìˆ˜ì¶•
        """
        return np.sign(coeffs) * np.maximum(np.abs(coeffs) - threshold, 0)
    
    def forward(self, pred, target):
        """
        pred, target: [B, 1, H, W] - normalized to [0, 1]
        """
        batch_size = pred.size(0)
        device = pred.device
        total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        valid_samples = 0
        
        for i in range(batch_size):
            # Move to CPU for pywt
            pred_np = pred[i, 0].detach().cpu().numpy()
            target_np = target[i, 0].detach().cpu().numpy()
            
            # Sanity checks
            if np.isnan(pred_np).any() or np.isnan(target_np).any():
                continue
            if np.isinf(pred_np).any() or np.isinf(target_np).any():
                continue
            
            try:
                # Multi-level DWT decomposition
                coeffs_pred = pywt.wavedec2(pred_np, self.wavelet, level=self.levels)
                coeffs_target = pywt.wavedec2(target_np, self.wavelet, level=self.levels)
                
                # coeffs = [cA_n, (cH_n, cV_n, cD_n), ..., (cH_1, cV_1, cD_1)]
                
                level_loss = 0
                
                for level_idx in range(1, len(coeffs_pred)):  # Skip approximation (cA)
                    cH_pred, cV_pred, cD_pred = coeffs_pred[level_idx]
                    cH_target, cV_target, cD_target = coeffs_target[level_idx]
                    
                    # Adaptive threshold per level
                    # Higher levels (coarser) get lower threshold
                    level_threshold = self.threshold / (2 ** (level_idx - 1))
                    
                    # Normalize threshold to match [0, 1] range if needed
                    if self.normalize_threshold:
                        # Input is [0, 1], so scale threshold accordingly
                        level_threshold = level_threshold / 255.0
                    
                    # Apply Soft Thresholding to high-frequency components
                    cH_pred_t = self.soft_threshold(cH_pred, level_threshold)
                    cV_pred_t = self.soft_threshold(cV_pred, level_threshold)
                    cD_pred_t = self.soft_threshold(cD_pred, level_threshold)
                    
                    cH_target_t = self.soft_threshold(cH_target, level_threshold)
                    cV_target_t = self.soft_threshold(cV_target, level_threshold)
                    cD_target_t = self.soft_threshold(cD_target, level_threshold)
                    
                    # Convert to tensors
                    cH_pred_tensor = torch.from_numpy(cH_pred_t).float().to(device)
                    cH_target_tensor = torch.from_numpy(cH_target_t).float().to(device)
                    
                    cV_pred_tensor = torch.from_numpy(cV_pred_t).float().to(device)
                    cV_target_tensor = torch.from_numpy(cV_target_t).float().to(device)
                    
                    cD_pred_tensor = torch.from_numpy(cD_pred_t).float().to(device)
                    cD_target_tensor = torch.from_numpy(cD_target_t).float().to(device)
                    
                    # Compute L1 loss on thresholded coefficients
                    loss_h = F.l1_loss(cH_pred_tensor, cH_target_tensor)
                    loss_v = F.l1_loss(cV_pred_tensor, cV_target_tensor)
                    loss_d = F.l1_loss(cD_pred_tensor, cD_target_tensor)
                    
                    # Check for NaN
                    if torch.isnan(loss_h) or torch.isnan(loss_v) or torch.isnan(loss_d):
                        continue
                    
                    # Weight by level (finer levels get higher weight)
                    level_weight = 1.0 / level_idx
                    level_loss = level_loss + level_weight * (loss_h + loss_v + loss_d) / 3.0
                
                total_loss = total_loss + level_loss
                valid_samples += 1
                
            except Exception as e:
                print(f"âš ï¸ Wavelet error in sample {i}: {e}")
                continue
        
        if valid_samples == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return total_loss / valid_samples


class SSIMLoss(nn.Module):
    """SSIM Loss (Fixed for stability)"""
    def __init__(self, window_size=11):
        super().__init__()
        self.window_size = window_size
        self.C1 = 0.01 ** 2
        self.C2 = 0.03 ** 2
    
    def gaussian_window(self, window_size, sigma=1.5):
        gauss = torch.exp(torch.tensor([
            -((x - window_size//2)**2) / (2.0 * sigma**2) 
            for x in range(window_size)
        ], dtype=torch.float32))
        return gauss / gauss.sum()
    
    def forward(self, pred, target):
        """
        pred, target: [B, 1, H, W]
        """
        # Check for nan/inf
        if torch.isnan(pred).any() or torch.isnan(target).any():
            print("âš ï¸ Warning: NaN in SSIM input")
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        if torch.isinf(pred).any() or torch.isinf(target).any():
            print("âš ï¸ Warning: Inf in SSIM input")
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # Create Gaussian window
        window_1d = self.gaussian_window(self.window_size).unsqueeze(0)
        window = window_1d.mm(window_1d.t()).unsqueeze(0).unsqueeze(0)
        window = window.to(pred.device)
        
        mu1 = F.conv2d(pred, window, padding=self.window_size//2)
        mu2 = F.conv2d(target, window, padding=self.window_size//2)
        
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.conv2d(pred * pred, window, padding=self.window_size//2) - mu1_sq
        sigma2_sq = F.conv2d(target * target, window, padding=self.window_size//2) - mu2_sq
        sigma12 = F.conv2d(pred * target, window, padding=self.window_size//2) - mu1_mu2
        
        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \
                   ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))
        
        ssim_loss = 1 - ssim_map.mean()
        
        # Clamp to prevent extreme values
        ssim_loss = torch.clamp(ssim_loss, 0, 2)
        
        return ssim_loss


class CombinedLoss(nn.Module):
    """
    L1 + SSIM + Wavelet (with Soft Thresholding)
    
    ë…¼ë¬¸ ê¸°ë°˜ ê°œì„  + Blurring ë°©ì§€ ê°•í™”:
    - Wavelet Lossì— Soft Thresholding ì ìš©
    - Multi-level DWTë¡œ ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ì²˜ë¦¬
    - Adaptive thresholdë¡œ ë ˆë²¨ë³„ ìµœì í™”
    - SSIMìœ¼ë¡œ êµ¬ì¡° ë³´ì¡´ ê°•í™”
    
    NEW: Learnable Loss Weights (Uncertainty-based)
    - learn_weights=True: weightê°€ ìë™ìœ¼ë¡œ í•™ìŠµë¨
    - learn_weights=False: ê³ ì • weight (ê¸°ì¡´ ë°©ì‹)
    """
    def __init__(self, l1_weight=1.0, ssim_weight=0.5, wavelet_weight=0.1, 
                 wavelet_threshold=50, wavelet_levels=3, learn_weights=False):
        super().__init__()
        
        self.learn_weights = learn_weights
        
        self.l1_loss = nn.L1Loss()
        self.ssim_loss = SSIMLoss()
        self.wavelet_loss = WaveletLoss(
            wavelet='haar',
            threshold=wavelet_threshold,  # ë…¼ë¬¸ì˜ ìµœì ê°’: 50
            levels=wavelet_levels,  # 3-levelë¡œ ê°•í™”
            normalize_threshold=True
        )
        
        if learn_weights:
            # Learnable weights (uncertainty-based)
            # Reference: Kendall et al., CVPR 2018
            # log_var = log(1/weight) â†’ weightê°€ ì»¤ì§€ë©´ log_var ì‘ì•„ì§
            self.log_var_l1 = nn.Parameter(torch.zeros(1))
            self.log_var_ssim = nn.Parameter(torch.log(torch.tensor(l1_weight / ssim_weight)))
            self.log_var_wavelet = nn.Parameter(torch.log(torch.tensor(l1_weight / wavelet_weight)))
            
            print(f"\nğŸ“Š CombinedLoss (Learnable Weights - ìë™ ìµœì í™”!)")
            print(f"   ì´ˆê¸° L1 weight: {l1_weight:.2f}")
            print(f"   ì´ˆê¸° SSIM weight: {ssim_weight:.2f}")
            print(f"   ì´ˆê¸° Wavelet weight: {wavelet_weight:.2f}")
            print(f"   Wavelet threshold: {wavelet_threshold}")
            print(f"   Wavelet levels: {wavelet_levels}")
            print(f"   â†’ Weightê°€ validation loss ë³´ê³  ìë™ ì¡°ì •ë©ë‹ˆë‹¤!")
        else:
            # Fixed weights (ê¸°ì¡´ ë°©ì‹)
            self.l1_weight = l1_weight
            self.ssim_weight = ssim_weight
            self.wavelet_weight = wavelet_weight
            
            print(f"\nğŸ“Š CombinedLoss Configuration (Blurring ë°©ì§€ ëª¨ë“œ):")
            print(f"   L1 weight: {l1_weight} (ê³ ì •)")
            print(f"   SSIM weight: {ssim_weight} (êµ¬ì¡° ë³´ì¡´)")
            print(f"   Wavelet weight: {wavelet_weight} (Edge ë³´ì¡´)")
            print(f"   Wavelet threshold: {wavelet_threshold}")
            print(f"   Wavelet levels: {wavelet_levels}")
            print(f"   â†’ ClariPI ëŒ€ë¹„ ì°¨ë³„í™”: Sharp Edge ìœ ì§€!")
    
    def get_current_weights(self):
        """í˜„ì¬ effective weight ë°˜í™˜ (learnableì¼ ë•Œë§Œ ì˜ë¯¸ìˆìŒ)"""
        if self.learn_weights:
            # weight = 1 / (2 * exp(log_var))
            w_l1 = 1.0 / (2 * torch.exp(self.log_var_l1))
            w_ssim = 1.0 / (2 * torch.exp(self.log_var_ssim))
            w_wavelet = 1.0 / (2 * torch.exp(self.log_var_wavelet))
            
            return {
                'l1': w_l1.item(),
                'ssim': w_ssim.item(),
                'wavelet': w_wavelet.item()
            }
        else:
            return {
                'l1': self.l1_weight,
                'ssim': self.ssim_weight,
                'wavelet': self.wavelet_weight
            }
    
    def forward(self, pred, target):
        # Clamp predictions to [0, 1]
        pred = torch.clamp(pred, 0, 1)
        
        # Check for nan/inf
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            print("âš ï¸ Warning: NaN/Inf in prediction!")
            return torch.tensor(float('inf'), device=pred.device), {
                'l1': float('inf'), 'ssim': float('inf'), 
                'wavelet': float('inf'), 'total': float('inf')
            }
        
        # L1 loss (always active)
        l1 = self.l1_loss(pred, target)
        
        # SSIM loss
        ssim = self.ssim_loss(pred, target)
        
        # Wavelet loss with Soft Thresholding
        wavelet = self.wavelet_loss(pred, target)
        
        # Check individual losses
        if torch.isnan(l1):
            l1 = torch.tensor(0.0, device=pred.device, requires_grad=True)
        if torch.isnan(ssim):
            ssim = torch.tensor(0.0, device=pred.device, requires_grad=True)
        if torch.isnan(wavelet):
            wavelet = torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        if self.learn_weights:
            # Uncertainty-weighted loss
            # loss = Î£ (loss_i * exp(-log_var_i) + log_var_i)
            precision_l1 = torch.exp(-self.log_var_l1)
            precision_ssim = torch.exp(-self.log_var_ssim)
            precision_wavelet = torch.exp(-self.log_var_wavelet)
            
            total = (precision_l1 * l1 + self.log_var_l1 +
                     precision_ssim * ssim + self.log_var_ssim +
                     precision_wavelet * wavelet + self.log_var_wavelet)
            
            # Get effective weights for logging
            weights = self.get_current_weights()
            
            return total, {
                'l1': l1.item(),
                'ssim': ssim.item(),
                'wavelet': wavelet.item(),
                'total': total.item(),
                'weight_l1': weights['l1'],
                'weight_ssim': weights['ssim'],
                'weight_wavelet': weights['wavelet']
            }
        else:
            # Fixed weights (ê¸°ì¡´ ë°©ì‹)
            total = (self.l1_weight * l1 + 
                     self.ssim_weight * ssim + 
                     self.wavelet_weight * wavelet)
            
            return total, {
                'l1': l1.item(),
                'ssim': ssim.item(),
                'wavelet': wavelet.item(),
                'total': total.item(),
                'weight_l1': self.l1_weight,
                'weight_ssim': self.ssim_weight,
                'weight_wavelet': self.wavelet_weight
            }

# ============================================================================
# Self-Supervised Loss (Ground Truth ë¶ˆí•„ìš”!)
# NC-CT ë°ì´í„°ë¥¼ ìœ„í•œ Self-Supervised Learning
# ============================================================================

class WaveletSparsityLoss(nn.Module):
    """
    Target ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•œ Wavelet Loss
    
    í•µì‹¬ ì›ë¦¬:
    - ê¹¨ë—í•œ ì´ë¯¸ì§€ì˜ wavelet ê³„ìˆ˜ëŠ” sparse (í¬ì†Œ)í•´ì•¼ í•¨
    - ì‘ì€ ê³„ìˆ˜(ë…¸ì´ì¦ˆ)ëŠ” 0ì— ê°€ê¹Œì›Œì•¼ í•¨
    - í° ê³„ìˆ˜(edge, êµ¬ì¡°)ëŠ” ìœ ì§€
    
    â†’ Soft Thresholdingì„ lossë¡œ ì§ì ‘ í•™ìŠµ!
    
    NO GROUND TRUTH NEEDED!
    """
    def __init__(self, threshold=50, wavelet='haar', levels=3, normalize_threshold=True):
        super().__init__()
        self.threshold = threshold
        self.wavelet = wavelet
        self.levels = levels
        self.normalize_threshold = normalize_threshold
        
        print(f"ğŸ“Š WaveletSparsityLoss Initialized (Self-Supervised!):")
        print(f"   Wavelet: {wavelet}")
        print(f"   Threshold: {threshold}")
        print(f"   Levels: {levels}")
        print(f"   â†’ Ground Truth ë¶ˆí•„ìš”!")
        
    def soft_threshold(self, coeffs, threshold):
        """Soft thresholding function"""
        return np.sign(coeffs) * np.maximum(np.abs(coeffs) - threshold, 0)
    
    def forward(self, pred):
        """
        pred: [B, 1, H, W] - normalized to [0, 1]
        
        NO TARGET NEEDED!
        """
        batch_size = pred.size(0)
        device = pred.device
        total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        valid_samples = 0
        
        for i in range(batch_size):
            pred_np = pred[i, 0].detach().cpu().numpy()
            
            # Sanity checks
            if np.isnan(pred_np).any() or np.isinf(pred_np).any():
                continue
            
            try:
                # DWT decomposition
                coeffs_pred = pywt.wavedec2(pred_np, self.wavelet, level=self.levels)
                
                level_loss = 0
                
                for level_idx in range(1, len(coeffs_pred)):  # Skip approximation
                    cH_pred, cV_pred, cD_pred = coeffs_pred[level_idx]
                    
                    # Adaptive threshold per level
                    level_threshold = self.threshold / (2 ** (level_idx - 1))
                    
                    # Normalize threshold
                    if self.normalize_threshold:
                        level_threshold = level_threshold / 255.0
                    
                    # Soft threshold - ì´ê²Œ "target"ì´ ë¨!
                    cH_target = self.soft_threshold(cH_pred, level_threshold)
                    cV_target = self.soft_threshold(cV_pred, level_threshold)
                    cD_target = self.soft_threshold(cD_pred, level_threshold)
                    
                    # Convert to tensors
                    cH_pred_tensor = torch.from_numpy(cH_pred).float().to(device)
                    cH_target_tensor = torch.from_numpy(cH_target).float().to(device)
                    
                    cV_pred_tensor = torch.from_numpy(cV_pred).float().to(device)
                    cV_target_tensor = torch.from_numpy(cV_target).float().to(device)
                    
                    cD_pred_tensor = torch.from_numpy(cD_pred).float().to(device)
                    cD_target_tensor = torch.from_numpy(cD_target).float().to(device)
                    
                    # Loss: í˜„ì¬ ê³„ìˆ˜ê°€ thresholded versionê³¼ ë¹„ìŠ·í•˜ë„ë¡
                    loss_h = F.l1_loss(cH_pred_tensor, cH_target_tensor.detach())
                    loss_v = F.l1_loss(cV_pred_tensor, cV_target_tensor.detach())
                    loss_d = F.l1_loss(cD_pred_tensor, cD_target_tensor.detach())
                    
                    if torch.isnan(loss_h) or torch.isnan(loss_v) or torch.isnan(loss_d):
                        continue
                    
                    # Weight by level
                    level_weight = 1.0 / level_idx
                    level_loss = level_loss + level_weight * (loss_h + loss_v + loss_d) / 3.0
                
                total_loss = total_loss + level_loss
                valid_samples += 1
                
            except Exception as e:
                print(f"âš ï¸ Wavelet sparsity error in sample {i}: {e}")
                continue
        
        if valid_samples == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return total_loss / valid_samples


class Noise2VoidLoss(nn.Module):
    """
    Noise2Void style reconstruction loss
    
    í•µì‹¬ ì›ë¦¬:
    - Randomí•˜ê²Œ ì„ íƒëœ í”½ì…€ë§Œ loss ê³„ì‚°
    - Blind spot networkì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ self-supervised
    - í•˜ì§€ë§Œ ì¼ë°˜ networkì—ì„œë„ regularization íš¨ê³¼
    """
    def __init__(self, mask_ratio=0.02):
        super().__init__()
        self.mask_ratio = mask_ratio
        
        print(f"ğŸ“Š Noise2VoidLoss Initialized:")
        print(f"   Mask ratio: {mask_ratio}")
        
    def forward(self, pred, noisy_input):
        """
        pred: ëª¨ë¸ ì¶œë ¥
        noisy_input: ë…¸ì´ì¦ˆ ì…ë ¥ (target ì•„ë‹˜! ê°™ì€ ë…¸ì´ì¦ˆ ì´ë¯¸ì§€)
        
        ì›ë¦¬: Random pixelì—ì„œë§Œ reconstruction í•™ìŠµ
        â†’ Self-supervised denoising
        """
        # Random mask ìƒì„±
        mask = torch.rand_like(pred) < self.mask_ratio
        
        # Masked ìœ„ì¹˜ì—ì„œë§Œ loss ê³„ì‚°
        if mask.sum() == 0:
            # No masked pixels
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        loss = F.l1_loss(pred[mask], noisy_input[mask])
        
        return loss


class SelfSupervisedCombinedLoss(nn.Module):
    """
    NC-CTë¥¼ ìœ„í•œ Self-Supervised Combined Loss
    
    NO GROUND TRUTH NEEDED!
    
    êµ¬ì„±:
    1. Noise2Void: reconstruction (regularization)
    2. Wavelet Sparsity: soft thresholding prior
    3. Total Variation: smoothness
    
    ì´ ì¡°í•©ìœ¼ë¡œ ground truth ì—†ì´ë„ denoising í•™ìŠµ ê°€ëŠ¥!
    """
    def __init__(self, 
                 n2v_weight=1.0,
                 wavelet_weight=0.2, 
                 tv_weight=0.01,
                 wavelet_threshold=50,
                 wavelet_levels=3):
        super().__init__()
        
        self.n2v_weight = n2v_weight
        self.wavelet_weight = wavelet_weight
        self.tv_weight = tv_weight
        
        self.n2v_loss = Noise2VoidLoss(mask_ratio=0.02)
        self.wavelet_loss = WaveletSparsityLoss(
            threshold=wavelet_threshold,
            levels=wavelet_levels,
            normalize_threshold=True
        )
        
        print(f"\nğŸ¯ SelfSupervisedCombinedLoss Configuration:")
        print(f"   N2V weight: {n2v_weight}")
        print(f"   Wavelet sparsity weight: {wavelet_weight}")
        print(f"   TV weight: {tv_weight}")
        print(f"   â†’ Ground Truth ë¶ˆí•„ìš”!")
        print(f"   â†’ NC-CTì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥!")
        
    def total_variation_loss(self, pred):
        """Total Variation for smoothness"""
        diff_h = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])
        diff_v = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])
        return torch.mean(diff_h) + torch.mean(diff_v)
    
    def forward(self, pred, noisy_input):
        """
        pred: ëª¨ë¸ ì¶œë ¥
        noisy_input: ë…¸ì´ì¦ˆ ì…ë ¥ (ground truth ì•„ë‹˜!)
        
        Returns: total_loss, loss_dict
        """
        # Clamp predictions
        pred = torch.clamp(pred, 0, 1)
        
        # Check for nan/inf
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            print("âš ï¸ Warning: NaN/Inf in prediction!")
            return torch.tensor(float('inf'), device=pred.device), {
                'n2v': float('inf'),
                'wavelet_sparsity': float('inf'),
                'tv': float('inf'),
                'total': float('inf')
            }
        
        # 1. Noise2Void reconstruction
        n2v = self.n2v_loss(pred, noisy_input)
        
        # 2. Wavelet sparsity (NO target!)
        wavelet = self.wavelet_loss(pred)
        
        # 3. Total variation
        tv = self.total_variation_loss(pred)
        
        # Check individual losses
        if torch.isnan(n2v):
            n2v = torch.tensor(0.0, device=pred.device, requires_grad=True)
        if torch.isnan(wavelet):
            wavelet = torch.tensor(0.0, device=pred.device, requires_grad=True)
        if torch.isnan(tv):
            tv = torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # Combined
        total = (self.n2v_weight * n2v + 
                 self.wavelet_weight * wavelet + 
                 self.tv_weight * tv)
        
        return total, {
            'n2v': n2v.item(),
            'wavelet_sparsity': wavelet.item(),
            'tv': tv.item(),
            'total': total.item(),
            'weight_n2v': self.n2v_weight,
            'weight_wavelet': self.wavelet_weight,
            'weight_tv': self.tv_weight
        }